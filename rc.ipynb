{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.patches as patches\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video config\n",
    "WIDTH = 200\n",
    "HEIGHT = 200\n",
    "DIM = 3\n",
    "X_DIM = 3\n",
    "\n",
    "WIDTH_FACTOR = 800 // WIDTH\n",
    "HEIGHT_FACTOR = 600 // HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESN:\n",
    "    def __init__(self, input_size, reservoir_size, output_size, spectral_radius=0.9, alpha=0.99):\n",
    "        self.input_size = input_size\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.output_size = output_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W_in = np.random.rand(reservoir_size, input_size + 1) - 0.5  # bias term\n",
    "        self.W_res = np.random.rand(reservoir_size, reservoir_size) - 0.5 # bias term\n",
    "        self.W_out = np.random.rand(output_size, reservoir_size) - 0.5    # bias term\n",
    "\n",
    "        self.W_res *= spectral_radius / np.max(np.abs(np.linalg.eigvals(self.W_res)))\n",
    "\n",
    "    def train(self, X_train, y_train, transient=100):\n",
    "        X_train = np.concatenate((np.ones((len(X_train), 1)), X_train), axis=1)  # Add bias term to input\n",
    "        X_res = np.zeros((len(X_train), self.reservoir_size))\n",
    "        x = np.zeros(self.reservoir_size)\n",
    "\n",
    "        for t in range(len(X_train)):\n",
    "            u = X_train[t]\n",
    "            x = (1 - self.alpha) * x + self.alpha * np.tanh(np.dot(self.W_in, u) + np.dot(self.W_res, x))\n",
    "            if t > transient:\n",
    "                X_res[t] = x\n",
    "\n",
    "        self.W_out = np.dot(np.linalg.pinv(X_res[transient:]), y_train[transient:])\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.concatenate((np.ones((len(X_test), 1)), X_test), axis=1)  # Add bias term to input\n",
    "        X_res = np.zeros((len(X_test), self.reservoir_size))\n",
    "        x = np.zeros(self.reservoir_size)\n",
    "\n",
    "        for t in range(len(X_test)):\n",
    "            u = X_test[t]\n",
    "            x = (1 - self.alpha) * x + self.alpha * np.tanh(np.dot(self.W_in, u) + np.dot(self.W_res, x))\n",
    "            X_res[t] = x\n",
    "\n",
    "        return np.dot(X_res, self.W_out)\n",
    "    \n",
    "    def identity(self, x):\n",
    "        return softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes_from_array(video_path, bounding_boxes, output_video_path):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the codec and create a VideoWriter object for AVI format\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Read each frame, draw bounding boxes, and write to output video\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract coordinates for the current frame\n",
    "        if frame_count < len(bounding_boxes):\n",
    "            x, y = bounding_boxes[frame_count]\n",
    "            x, y = int(x), int(y)\n",
    "            width, height = 30, 30  # Assumed width and height\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"Bounding boxes added and new video saved to: {output_video_path}\")\n",
    "\n",
    "def read_png_image(file_path, name, i):\n",
    "\n",
    "    file_name = os.path.basename(file_path)\n",
    "    image_array = random_augment(file_path)\n",
    "    #image_array = resize(image_array, (HEIGHT , WIDTH), anti_aliasing=True)\n",
    "\n",
    "    number = int(file_name.split(\"_\")[1].split(\".\")[0])\n",
    "    df = pd.read_csv(f'assets/preprocessed_bb_coordinates/moving_{name}_{i}.csv')\n",
    "    x = df['X-coordinate']\n",
    "    y = df['Y-coordinate']\n",
    "\n",
    "    # Get object coordinates\n",
    "    x_coord = int(x[number] // WIDTH_FACTOR)\n",
    "    y_coord = int(y[number] // HEIGHT_FACTOR)\n",
    "\n",
    "    image_array = crop_image(image_array, x_coord, y_coord)\n",
    "\n",
    "    return image_array, x_coord, y_coord\n",
    "\n",
    "def plot_image(img, bb, save_path):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    x, y = (bb[0]-30)//WIDTH_FACTOR, (bb[1]-30)//HEIGHT_FACTOR\n",
    "    rect = patches.Rectangle((x, y), 10, 10, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # Save the figure instead of showing it\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()  # Close the figure to free memory\n",
    "\n",
    "def zoom(image, scale=(1, 1.3)):\n",
    "  zoom = iaa.Affine(scale=scale)\n",
    "  image = zoom.augment_image(image)\n",
    "  return image\n",
    "\n",
    "def pan(image):\n",
    "  pan = iaa.Affine(translate_percent={\"x\":(-0.08, 0.08), \"y\":(-0.08, 0.08)})\n",
    "  image = pan.augment_image(image)\n",
    "  return image\n",
    "\n",
    "def img_random_brightness(image):\n",
    "  # As of experience models recats better to darker immages\n",
    "  brightness = iaa.Multiply((0.2, 1.2))\n",
    "  image = brightness.augment_image(image)\n",
    "  return image\n",
    "\n",
    "def random_augment(image):\n",
    "  \n",
    "  image = mpimg.imread(image)\n",
    "  #if np.random.rand() < 0.5:\n",
    "  #  image = pan(image)\n",
    "  if np.random.rand() < 0.5:\n",
    "    image = zoom(image)\n",
    "  if np.random.rand() < 0.5:\n",
    "    image = img_random_brightness(image)\n",
    "  \n",
    "  return image\n",
    "\n",
    "def crop_image(image_array, x, y, output_size=(201, 201)):\n",
    "    \n",
    "    # Define the cropping box based on the object's location and the offset\n",
    "    left = max(0, x - 100)\n",
    "    upper = max(0, y - 100)\n",
    "    right = min(WIDTH, x + 100)\n",
    "    lower = min(HEIGHT, y + 100)\n",
    "    \n",
    "    # Crop the image array\n",
    "    cropped_image_array = image_array[upper:lower, left:right]\n",
    "    \n",
    "    # Pad the cropped image to ensure consistent size\n",
    "    pad_height = max(0, output_size[0] - cropped_image_array.shape[0])\n",
    "    pad_width = max(0, output_size[1] - cropped_image_array.shape[1])\n",
    "    top_pad = pad_height // 2\n",
    "    bottom_pad = pad_height - top_pad\n",
    "    left_pad = pad_width // 2\n",
    "    right_pad = pad_width - left_pad\n",
    "    cropped_image_array = np.pad(cropped_image_array, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode='constant')\n",
    "    \n",
    "    # Return the cropped and padded image array\n",
    "    return cropped_image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X = X / 255.0  # Normalize pixel values to range [0, 1]\n",
    "X = np.array(X)\n",
    "y = OneHotEncoder().fit_transform(y.values.reshape(-1, 1)).toarray()  # One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image(image_array):\n",
    "    \"\"\"\n",
    "    Plot an image from a NumPy array.\n",
    "    \n",
    "    Parameters:\n",
    "        image_array (numpy.ndarray): The image array to be plotted.\n",
    "    \"\"\"\n",
    "    plt.imshow(image_array.reshape(HEIGHT, WIDTH), cmap='gray')  # Assuming grayscale image, change cmap for color images\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (70000, 784)\n",
      "y shape: (70000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train ESN\n",
    "input_size = X_train.shape[1]\n",
    "reservoir_size = 1000\n",
    "output_size = 10\n",
    "esn = ESN(input_size, reservoir_size, output_size)\n",
    "esn.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9204285714285714\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate\n",
    "predictions = esn.predict(X_test)\n",
    "accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ground truth label:\", y_test[90])\n",
    "print(\"Predicted label:\", predictions[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_softmax = softmax(predictions[90])\n",
    "print(\"Predicted probabilities:  \", np.argmax(prediction_softmax))\n",
    "print(\"ground truth label:       \", np.argmax(y_test[90]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes (External data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images from assets/shapes/geometric shapes dataset/Circle:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images from assets/shapes/geometric shapes dataset/Circle:  50%|████▉     | 4999/10000 [00:02<00:02, 1746.74it/s]\n",
      "Loading images from assets/shapes/geometric shapes dataset/Square:  50%|████▉     | 4999/10000 [00:02<00:02, 2042.47it/s]\n",
      "Loading images from assets/shapes/geometric shapes dataset/Triangle:  50%|████▉     | 4999/10000 [00:02<00:02, 1887.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2]\n",
      "(15000, 14400)\n",
      "(15000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Define your folders\n",
    "main_folder = 'assets/shapes/geometric shapes dataset'\n",
    "Circle = os.path.join(main_folder, 'Circle')\n",
    "Square = os.path.join(main_folder, 'Square')\n",
    "Triangle = os.path.join(main_folder, 'Triangle')\n",
    "folders = [Circle, Square, Triangle]\n",
    "\n",
    "# Function to load images from a folder\n",
    "def load_images(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    f = 0\n",
    "    for filename in tqdm(os.listdir(folder), desc=f\"Loading images from {folder}\"):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            img = Image.open(os.path.join(folder, filename))\n",
    "            # Resize image to a fixed size if necessary\n",
    "            img = img.resize((HEIGHT, WIDTH))\n",
    "            img = np.array(img)\n",
    "            img = img[40:160, 40:160, :]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "        f += 1\n",
    "        if f == 5000:\n",
    "            break\n",
    "    return images, labels\n",
    "\n",
    "# Load images and labels\n",
    "X = []\n",
    "y = []\n",
    "n = 0\n",
    "for folder in folders:\n",
    "    images, labels = load_images(folder, n)\n",
    "    X.extend(images)\n",
    "    y.extend(labels)\n",
    "    n += 1\n",
    "\n",
    "# get unique labels\n",
    "unique_labels = np.unique(y)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(X)\n",
    "X = np.reshape(X, (len(X), (120*120), DIM))\n",
    "X_gray = np.dot(X[...,:DIM], np.array([0.2989, 0.5870, 0.1140]))\n",
    "X = np.expand_dims(X_gray, axis=-1)\n",
    "X = X[:, :, 0]\n",
    "print(X.shape)\n",
    "X = X / 255.0\n",
    "#X = pd.DataFrame(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Perform one-hot encoding on labels\n",
    "y_encoded = OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray()\n",
    "print(y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image(image_array):\n",
    "    \"\"\"\n",
    "    Plot an image from a NumPy array.\n",
    "    \n",
    "    Parameters:\n",
    "        image_array (numpy.ndarray): The image array to be plotted.\n",
    "    \"\"\"\n",
    "    plt.imshow(image_array.reshape(120, 120), cmap='gray')  # Assuming grayscale image, change cmap for color images\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n",
    "\n",
    "plot_image(X[14000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 14400), (15000, 3))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train ESN\n",
    "input_size = X_train.shape[1]\n",
    "reservoir_size = 2000\n",
    "output_size = y_train.shape[1]\n",
    "esn = ESN(input_size, reservoir_size, output_size)\n",
    "esn.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.478\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate\n",
    "predictions = esn.predict(X_test)\n",
    "accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth label: [1. 0. 0.]\n",
      "Predicted label:    [ 0.54312134  0.48158646 -0.02485657]\n"
     ]
    }
   ],
   "source": [
    "TEST = 470\n",
    "print(\"Ground truth label:\", y_test[TEST])\n",
    "print(\"Predicted label:   \", predictions[TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities:   0\n",
      "ground truth label:        0\n"
     ]
    }
   ],
   "source": [
    "prediction_softmax = softmax(predictions[TEST])\n",
    "print(\"Predicted probabilities:  \", np.argmax(prediction_softmax))\n",
    "print(\"ground truth label:       \", np.argmax(y_test[TEST]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapes (Original frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video frames\n",
    "shapes = {'circle':3, 'rect':3}\n",
    "#shapes = {'circle':1}\n",
    "X, y = [], []\n",
    "n = 0\n",
    "\n",
    "for j in shapes.keys():\n",
    "    print(f'Preprocessing images for shape: {j}')\n",
    "    for i in tqdm(range(shapes[j]), desc='Preprocessing images...'):\n",
    "        img_path = os.path.join(os.getcwd(), f'assets/original_frames/moving_{j}_{i}')\n",
    "        files = glob.glob(os.path.join(img_path, '*.png'))\n",
    "        sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "\n",
    "        for filepath in sorted_files:\n",
    "            image_array, _, _ = read_png_image(filepath, j, i)\n",
    "\n",
    "            X.append(image_array)\n",
    "            y.append(n)\n",
    "    n += 1\n",
    "\n",
    "print('Preprocessing done!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X, (len(X), HEIGHT*WIDTH, DIM))\n",
    "X_gray = np.dot(X[...,:DIM], [0.2989, 0.5870, 0.1140])\n",
    "X = np.expand_dims(X_gray, axis=-1)\n",
    "\n",
    "y = OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray()  # One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "n_samples = len(X)\n",
    "train_idx = int(n_samples * 0.95)\n",
    "X_train, X_test = X[:train_idx,...], X[train_idx:, ...]\n",
    "y_train, y_test = y[:train_idx], y[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train ESN\n",
    "input_size = X_train.shape[1]\n",
    "reservoir_size = 1000\n",
    "output_size = 3\n",
    "esn = ESN(input_size, \n",
    "          reservoir_size, \n",
    "          output_size, \n",
    "          spectral_radius=0.9, \n",
    "          alpha=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn.train(np.array(X_train[:, :, 0]).reshape(len(X_train), -1), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "predictions = esn.predict(np.array(X_test[:, :, 0]).reshape(len(X_test), -1))\n",
    "accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = 4\n",
    "print(\"Ground truth label:\", y_test[TEST])\n",
    "print(\"Predicted label:\", predictions[TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_softmax = softmax(predictions[TEST])\n",
    "print(\"Predicted probabilities:  \", np.argmax(prediction_softmax))\n",
    "print(\"ground truth label:       \", np.argmax(y_test[TEST]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
